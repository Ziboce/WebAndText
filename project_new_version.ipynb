{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of train data sample: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Dimention of train_data : torch.Size([16124, 1382])\n",
      "Example of the inverted encoding: [['looking']\n",
      " ['for']\n",
      " ['some']\n",
      " ['education']\n",
      " ['made']]\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "with open(\"adele.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Split the text (I consider whitespaces, points and commas as word here) + remove the empty string \"\"\n",
    "# data = np.array(list(filter(lambda a: a != \"\", re.split('(\\W)', text))))\n",
    "data = np.array(list(filter(lambda a: a != \"\", map(lambda x: x.lower(), re.split('[^a-zA-Z\\.]', text)))))\n",
    "\n",
    "# Check that the empty string is correctly removed\n",
    "if len(data[data == \"\"]) != 0:\n",
    "    raise Exception(\"The empty string wasn't proprely removed from the data\")\n",
    "\n",
    "n = int(0.8 * len(data))\n",
    "\n",
    "# Create the encoder and set the categories on the training set\n",
    "encoder = OneHotEncoder().fit(data.reshape(-1,1))\n",
    "\n",
    "# Check the number of categories of the encoder is the same than the different words in the corpus\n",
    "if len(encoder.categories_[0]) != len(set(data)):\n",
    "    raise Exception(f\"Encoder categories counts {len(encoder.categories_[0])} don't match the value of differents words {len(set(data))}\")\n",
    "\n",
    "vocab_size = len(set(data))\n",
    "\n",
    "train_data = torch.Tensor(encoder.transform(data[:n].reshape(-1,1)).toarray())\n",
    "val_data = torch.Tensor(encoder.transform(data[n:].reshape(-1,1)).toarray())\n",
    "\n",
    "print(f\"Example of train data sample: {train_data[0:5]}\")\n",
    "print(f\"Dimention of train_data : {train_data.shape}\")\n",
    "print(f\"Example of the inverted encoding: {encoder.inverse_transform(train_data[0:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcul de weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de poids : [('looking', 0.08333333333333333), ('for', 0.006896551724137931), ('some', 0.06666666666666667), ('education', 0.5), ('made', 0.03333333333333333), ('my', 0.002702702702702703), ('way', 0.023255813953488372), ('into', 0.045454545454545456), ('the', 0.0015822784810126582), ('night', 0.038461538461538464)]\n"
     ]
    }
   ],
   "source": [
    "word_counts = Counter(data)  # Compte les occurrences de chaque mot\n",
    "total_words = sum(word_counts.values())  # Nombre total de mots dans le corpus\n",
    "\n",
    "# 2. Calcul des poids inverses pour chaque mot\n",
    "word_weights = {word: 1.0 / freq for word, freq in word_counts.items()}  # Inverse de la fréquence\n",
    "\n",
    "# Normaliser les poids (optionnel, mais recommandé pour éviter des écarts extrêmes)\n",
    "max_weight = max(word_weights.values())\n",
    "word_weights = {word: weight / max_weight for word, weight in word_weights.items()}\n",
    "\n",
    "# Afficher quelques exemples de poids\n",
    "print(f\"Exemple de poids : {list(word_weights.items())[:10]}\")\n",
    "\n",
    "# Exemple : transformer les poids en un vecteur aligné avec l'encodeur\n",
    "word_to_idx = {word: idx for idx, word in enumerate(encoder.categories_[0])}  # Associer chaque mot à son index\n",
    "weights_array = np.array([word_weights[word] for word in encoder.categories_[0]])  # Créer un tableau de poids\n",
    "\n",
    "# Transformer les poids en tenseur PyTorch pour une utilisation dans la fonction de perte\n",
    "weights_tensor = torch.Tensor(weights_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is arbitrary values\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "N_EPOCHS = 10\n",
    "LR = 3e-4\n",
    "BATCH_SIZE = 32\n",
    "SEQ_LEN = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence(data, seq_len):\n",
    "    n = len(data)\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(n - seq_len):\n",
    "        X.append(data[i:i+seq_len])\n",
    "        y.append(data[i+1:i+seq_len+1])\n",
    "    return X, y\n",
    "\n",
    "train_X, train_y = create_sequence(train_data, SEQ_LEN)\n",
    "\n",
    "class Text(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "train_dataset = Text(train_X, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout, model_type='LSTM'):\n",
    "        super(NLP, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.state_dim = hidden_dim\n",
    "        self.num_layers = n_layers\n",
    "        self.rnn_type = model_type\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        if model_type == 'LSTM':\n",
    "            self.nlp = nn.LSTM(vocab_size, hidden_dim, n_layers, batch_first=True, dropout=dropout)\n",
    "        elif model_type == 'GRU':\n",
    "            self.nlp = nn.GRU(vocab_size, hidden_dim, n_layers, batch_first=True, dropout=dropout)\n",
    "        else:\n",
    "            raise Exception(\"Model type not supported\")\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        # x = [batch_size, seq_len, vocab_size]\n",
    "        # print(x.shape)\n",
    "        x, hidden = self.nlp(x)\n",
    "        # x = [batch_size, seq_len, state_dim]\n",
    "        # print(x.shape)\n",
    "        x = self.fc(x)\n",
    "        # x = [batch_size, seq_len, vocab_size]\n",
    "        # print(x.shape)\n",
    "        return x, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            # LSTM requires both hidden state and cell state\n",
    "            hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(self.device)\n",
    "            cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(self.device)\n",
    "            return (hidden, cell)\n",
    "        else:\n",
    "            # GRU only requires the hidden state\n",
    "            hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(self.device)\n",
    "            return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n",
      "Epoch 0, step 0, loss 7.234217166900635\n",
      "Epoch 0, step 100, loss 5.5404157638549805\n",
      "Epoch 0, step 200, loss 5.54812479019165\n",
      "Epoch 0, step 300, loss 5.535862445831299\n",
      "Epoch 0, step 400, loss 5.54105806350708\n",
      "Epoch 0, step 500, loss 5.531286239624023\n",
      "Epoch 0 finished. Train loss: 5.622061864993207, Perplexity: 276.4588167418433\n",
      "Epoch 1, step 0, loss 5.413279056549072\n",
      "Epoch 1, step 100, loss 5.422394752502441\n",
      "Epoch 1, step 200, loss 5.346199035644531\n",
      "Epoch 1, step 300, loss 5.5287628173828125\n",
      "Epoch 1, step 400, loss 5.641982555389404\n",
      "Epoch 1, step 500, loss 5.512269020080566\n",
      "Epoch 1 finished. Train loss: 5.483637000173034, Perplexity: 240.72061813394512\n",
      "Epoch 2, step 0, loss 5.399675369262695\n",
      "Epoch 2, step 100, loss 5.457913398742676\n",
      "Epoch 2, step 200, loss 5.459787845611572\n",
      "Epoch 2, step 300, loss 5.427606105804443\n",
      "Epoch 2, step 400, loss 5.318498611450195\n",
      "Epoch 2, step 500, loss 5.3667521476745605\n",
      "Epoch 2 finished. Train loss: 5.419261882129768, Perplexity: 225.71245860157063\n",
      "Epoch 3, step 0, loss 5.291538715362549\n",
      "Epoch 3, step 100, loss 5.0996785163879395\n",
      "Epoch 3, step 200, loss 5.18093204498291\n",
      "Epoch 3, step 300, loss 5.116980075836182\n",
      "Epoch 3, step 400, loss 5.097729206085205\n",
      "Epoch 3, step 500, loss 5.030144691467285\n",
      "Epoch 3 finished. Train loss: 5.130338674511161, Perplexity: 169.0743695289808\n",
      "Epoch 4, step 0, loss 5.0513081550598145\n",
      "Epoch 4, step 100, loss 4.82330322265625\n",
      "Epoch 4, step 200, loss 4.8979411125183105\n",
      "Epoch 4, step 300, loss 4.768228530883789\n",
      "Epoch 4, step 400, loss 4.837684154510498\n",
      "Epoch 4, step 500, loss 4.639097213745117\n",
      "Epoch 4 finished. Train loss: 4.799765441811108, Perplexity: 121.48191959761127\n",
      "Epoch 5, step 0, loss 4.394618511199951\n",
      "Epoch 5, step 100, loss 4.4998884201049805\n",
      "Epoch 5, step 200, loss 4.482213973999023\n",
      "Epoch 5, step 300, loss 4.464224338531494\n",
      "Epoch 5, step 400, loss 4.434788227081299\n",
      "Epoch 5, step 500, loss 4.371074199676514\n",
      "Epoch 5 finished. Train loss: 4.5189250221783315, Perplexity: 91.73692978964934\n",
      "Epoch 6, step 0, loss 4.241742134094238\n",
      "Epoch 6, step 100, loss 4.363372802734375\n",
      "Epoch 6, step 200, loss 4.262441158294678\n",
      "Epoch 6, step 300, loss 4.171628475189209\n",
      "Epoch 6, step 400, loss 4.391258716583252\n",
      "Epoch 6, step 500, loss 4.110380172729492\n",
      "Epoch 6 finished. Train loss: 4.282799956812774, Perplexity: 72.44299355774749\n",
      "Epoch 7, step 0, loss 4.396659851074219\n",
      "Epoch 7, step 100, loss 4.092106342315674\n",
      "Epoch 7, step 200, loss 4.080287456512451\n",
      "Epoch 7, step 300, loss 4.110087871551514\n",
      "Epoch 7, step 400, loss 3.941258668899536\n",
      "Epoch 7, step 500, loss 3.978898286819458\n",
      "Epoch 7 finished. Train loss: 4.066923640120337, Perplexity: 58.37709710883141\n",
      "Epoch 8, step 0, loss 4.004777908325195\n",
      "Epoch 8, step 100, loss 4.016100883483887\n",
      "Epoch 8, step 200, loss 3.814574956893921\n",
      "Epoch 8, step 300, loss 3.819101572036743\n",
      "Epoch 8, step 400, loss 3.851090431213379\n",
      "Epoch 8, step 500, loss 3.4483067989349365\n",
      "Epoch 8 finished. Train loss: 3.8577767333263906, Perplexity: 47.35994045670776\n",
      "Epoch 9, step 0, loss 3.767058849334717\n",
      "Epoch 9, step 100, loss 3.606356382369995\n",
      "Epoch 9, step 200, loss 3.6499383449554443\n",
      "Epoch 9, step 300, loss 3.4834771156311035\n",
      "Epoch 9, step 400, loss 3.493441581726074\n",
      "Epoch 9, step 500, loss 3.3105475902557373\n",
      "Epoch 9 finished. Train loss: 3.645525737026814, Perplexity: 38.30290481433057\n"
     ]
    }
   ],
   "source": [
    "def train(model, dataloader, n_epochs, lr, batch_size, seq_len):\n",
    "    # Setup GPU related variables\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"device = {device}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_losses = []\n",
    "        for i, (X, y) in enumerate(dataloader):\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output, hidden = model(X, hidden)\n",
    "\n",
    "            # print(output.shape, y.shape)\n",
    "            output = output.view(-1, vocab_size)\n",
    "            y = y.view(-1, vocab_size)\n",
    "\n",
    "            # print(output.shape, y.shape)\n",
    "            loss = criterion(output, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, step {i}, loss {loss.item()}\")\n",
    "        \n",
    "        print(f\"Epoch {epoch} finished. Train loss: {np.array(train_losses).mean()}, Perplexity: {np.exp(np.array(train_losses).mean())}\")\n",
    "\n",
    "model = NLP(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT, 'LSTM')\n",
    "\n",
    "train(model, train_loader, N_EPOCHS, LR, BATCH_SIZE, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so i m i\n",
      "i know there is no m you be a i i a and sky fall\n",
      "i m m be i and\n",
      "baby i don t\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, encoder, start_word, num_words=10, random_sample=False):\n",
    "    \"\"\"\n",
    "    Generate text based on the trained model output.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The trained PyTorch model.\n",
    "    - encoder: The OneHotEncoder used for encoding the words.\n",
    "    - start_word: The initial word to start generating text.\n",
    "    - num_words: Number of words to generate.\n",
    "    - random_sample: If True, sample from the distribution instead of taking the max probability.\n",
    "    \n",
    "    Returns:\n",
    "    - generated_text: The generated sequence of words.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    start_word = start_word.lower().split()\n",
    "    # Initialize the generated text with the start word\n",
    "    generated_words = start_word\n",
    "    \n",
    "    # Convert the start word to its one-hot encoded representation\n",
    "    input_word = encoder.transform(np.array(start_word).reshape(-1, 1)).toarray()\n",
    "    input_tensor = torch.Tensor(input_word).unsqueeze(0).to(model.device)  # Add batch dimension\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    \n",
    "    # Generate the specified number of words\n",
    "    for _ in range(num_words):\n",
    "        # Get the model output with the hidden state\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(input_tensor, hidden)  # Pass hidden state\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = torch.softmax(output, dim=-1).squeeze().cpu().numpy()\n",
    "\n",
    "        if len(probabilities.shape) != 2:\n",
    "            probabilities = probabilities.reshape((1,-1))\n",
    "\n",
    "        model_output = []\n",
    "        for probability in probabilities:\n",
    "            # Determine the next word\n",
    "            if random_sample:\n",
    "                next_index = np.random.choice(len(probability), p=probability)\n",
    "            else:\n",
    "                next_index = np.argmax(probability)\n",
    "\n",
    "            encoding = np.zeros_like(probability)\n",
    "            encoding[next_index] = 1\n",
    "            # Get the corresponding word from the encoder\n",
    "            next_word = encoder.inverse_transform(encoding.reshape(1, -1))[0][0]\n",
    "            \n",
    "            # Append the generated word to the list\n",
    "            model_output.append(next_word)\n",
    "\n",
    "        for word in model_output:\n",
    "            generated_words.append(word)\n",
    "\n",
    "        # Update the input tensor with the new word\n",
    "        input_tensor = encoder.transform(np.array(model_output).reshape(-1, 1)).toarray()\n",
    "        input_tensor = torch.Tensor(input_tensor).unsqueeze(0).to(model.device)  # Add batch dimension\n",
    "    \n",
    "    # Join the generated words into a single string\n",
    "    generated_text = ' '.join(generated_words)\n",
    "    return generated_text\n",
    "\n",
    "# Exemple d'utilisation\n",
    "generated_text = generate_text(model, encoder, start_word='So', num_words=3, random_sample=False)\n",
    "print(generated_text)\n",
    "\n",
    "generated_text = generate_text(model, encoder, start_word='I know there is no', num_words=2, random_sample=False)\n",
    "print(generated_text)\n",
    "\n",
    "generated_text = generate_text(model, encoder, start_word='I m', num_words=2, random_sample=False)\n",
    "print(generated_text)\n",
    "\n",
    "generated_text_random = generate_text(model, encoder, start_word='Baby', num_words=3, random_sample=True)\n",
    "print(generated_text_random)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WebAndText",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
