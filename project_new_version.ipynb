{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of train data sample: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Dimention of train_data : torch.Size([16124, 1382])\n",
      "Example of the inverted encoding: [['looking']\n",
      " ['for']\n",
      " ['some']\n",
      " ['education']\n",
      " ['made']]\n"
     ]
    }
   ],
   "source": [
    "with open(\"adele.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Split the text (I consider whitespaces, points and commas as word here) + remove the empty string \"\"\n",
    "# data = np.array(list(filter(lambda a: a != \"\", re.split('(\\W)', text))))\n",
    "data = np.array(list(filter(lambda a: a != \"\", map(lambda x: x.lower(), re.split('[^a-zA-Z\\.]', text)))))\n",
    "\n",
    "# Check that the empty string is correctly removed\n",
    "if len(data[data == \"\"]) != 0:\n",
    "    raise Exception(\"The empty string wasn't proprely removed from the data\")\n",
    "\n",
    "n = int(0.8 * len(data))\n",
    "\n",
    "# Create the encoder and set the categories on the training set\n",
    "encoder = OneHotEncoder().fit(data.reshape(-1,1))\n",
    "\n",
    "# Check the number of categories of the encoder is the same than the different words in the corpus\n",
    "if len(encoder.categories_[0]) != len(set(data)):\n",
    "    raise Exception(f\"Encoder categories counts {len(encoder.categories_[0])} don't match the value of differents words {len(set(data))}\")\n",
    "\n",
    "vocab_size = len(set(data))\n",
    "\n",
    "train_data = torch.Tensor(encoder.transform(data[:n].reshape(-1,1)).toarray())\n",
    "val_data = torch.Tensor(encoder.transform(data[n:].reshape(-1,1)).toarray())\n",
    "\n",
    "print(f\"Example of train data sample: {train_data[0:5]}\")\n",
    "print(f\"Dimention of train_data : {train_data.shape}\")\n",
    "print(f\"Example of the inverted encoding: {encoder.inverse_transform(train_data[0:5])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is arbitrary values\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "N_LAYERS = 1\n",
    "DROPOUT = 0.5\n",
    "N_EPOCHS = 5\n",
    "LR = 3e-4\n",
    "BATCH_SIZE = 32\n",
    "SEQ_LEN = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence(data, seq_len):\n",
    "    n = len(data)\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(n - seq_len):\n",
    "        X.append(data[i:i+seq_len])\n",
    "        y.append(data[i+1:i+seq_len+1])\n",
    "    return X, y\n",
    "\n",
    "train_X, train_y = create_sequence(train_data, SEQ_LEN)\n",
    "\n",
    "class Text(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "train_dataset = Text(train_X, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super(NLP, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.state_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(vocab_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = [batch_size, seq_len, vocab_size]\n",
    "        # print(x.shape)\n",
    "        x, _ = self.lstm(x)\n",
    "        # x = [batch_size, seq_len, state_dim]\n",
    "        # print(x.shape)\n",
    "        x = self.fc(x)\n",
    "        # x = [batch_size, seq_len, vocab_size]\n",
    "        # print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n",
      "Epoch 0, step 0, loss 7.234655857086182\n",
      "Epoch 0, step 100, loss 5.61555814743042\n",
      "Epoch 0, step 200, loss 5.565292835235596\n",
      "Epoch 0, step 300, loss 5.515486717224121\n",
      "Epoch 0, step 400, loss 5.595889568328857\n",
      "Epoch 0, step 500, loss 5.36691427230835\n",
      "Epoch 0 finished. Train loss: 5.658745363739802, Perplexity: 286.7886013646327\n",
      "Epoch 1, step 0, loss 5.325730323791504\n",
      "Epoch 1, step 100, loss 5.438483238220215\n",
      "Epoch 1, step 200, loss 5.254950046539307\n",
      "Epoch 1, step 300, loss 5.218662738800049\n",
      "Epoch 1, step 400, loss 5.281633377075195\n",
      "Epoch 1, step 500, loss 5.16405725479126\n",
      "Epoch 1 finished. Train loss: 5.301364306190142, Perplexity: 200.61031725616814\n",
      "Epoch 2, step 0, loss 5.28195858001709\n",
      "Epoch 2, step 100, loss 4.91642951965332\n",
      "Epoch 2, step 200, loss 4.980137348175049\n",
      "Epoch 2, step 300, loss 4.832734107971191\n",
      "Epoch 2, step 400, loss 4.7653679847717285\n",
      "Epoch 2, step 500, loss 4.786016464233398\n",
      "Epoch 2 finished. Train loss: 4.909591751591586, Perplexity: 135.58405113516778\n",
      "Epoch 3, step 0, loss 4.4807891845703125\n",
      "Epoch 3, step 100, loss 4.381380558013916\n",
      "Epoch 3, step 200, loss 4.5382080078125\n",
      "Epoch 3, step 300, loss 4.348844528198242\n",
      "Epoch 3, step 400, loss 4.164917945861816\n",
      "Epoch 3, step 500, loss 4.315542221069336\n",
      "Epoch 3 finished. Train loss: 4.3916352871161095, Perplexity: 80.77239710032202\n",
      "Epoch 4, step 0, loss 4.355289936065674\n",
      "Epoch 4, step 100, loss 4.070318222045898\n",
      "Epoch 4, step 200, loss 4.029554843902588\n",
      "Epoch 4, step 300, loss 3.916001319885254\n",
      "Epoch 4, step 400, loss 3.669219970703125\n",
      "Epoch 4, step 500, loss 3.7925643920898438\n",
      "Epoch 4 finished. Train loss: 3.896127977143699, Perplexity: 49.21153155053317\n"
     ]
    }
   ],
   "source": [
    "def train(model, dataloader, n_epochs, lr, batch_size, seq_len):\n",
    "    # Setup GPU related variables\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"device = {device}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_losses = []\n",
    "        for i, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "\n",
    "            # print(output.shape, y.shape)\n",
    "            output = output.view(-1, vocab_size)\n",
    "            y = y.view(-1, vocab_size)\n",
    "\n",
    "            # print(output.shape, y.shape)\n",
    "            loss = criterion(output, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, step {i}, loss {loss.item()}\")\n",
    "        \n",
    "        print(f\"Epoch {epoch} finished. Train loss: {np.array(train_losses).mean()}, Perplexity: {np.exp(np.array(train_losses).mean())}\")\n",
    "\n",
    "model = NLP(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "\n",
    "train(model, train_loader, N_EPOCHS, LR, BATCH_SIZE, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baby i m the sky fall when i m the sky fall when i m the sky fall when i m\n",
      "baby chance stars we far every wish i spin. version however so guess haunted walk with burn thank as sometimes free\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, encoder, start_word, num_words=10, random_sample=False):\n",
    "    \"\"\"\n",
    "    Generate text based on the trained model output.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The trained PyTorch model.\n",
    "    - encoder: The OneHotEncoder used for encoding the words.\n",
    "    - start_word: The initial word to start generating text.\n",
    "    - num_words: Number of words to generate.\n",
    "    - random_sample: If True, sample from the distribution instead of taking the max probability.\n",
    "    \n",
    "    Returns:\n",
    "    - generated_text: The generated sequence of words.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    start_word = start_word.lower()\n",
    "    # Initialize the generated text with the start word\n",
    "    generated_words = [start_word]\n",
    "    \n",
    "    # Convert the start word to its one-hot encoded representation\n",
    "    input_word = encoder.transform(np.array([[start_word]])).toarray()\n",
    "    input_tensor = torch.Tensor(input_word)\n",
    "\n",
    "    # Generate the specified number of words\n",
    "    for _ in range(num_words):\n",
    "        # Get the model output\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = torch.softmax(output, dim=1).numpy().flatten()\n",
    "        \n",
    "        # Determine the next word\n",
    "        if random_sample:\n",
    "            next_index = np.random.choice(len(probabilities), p=probabilities)\n",
    "        else:\n",
    "            next_index = np.argmax(probabilities)\n",
    "        \n",
    "        # Get the corresponding word from the encoder\n",
    "        next_word = encoder.inverse_transform(np.eye(len(probabilities))[next_index].reshape(1, -1))[0][0]\n",
    "        \n",
    "        # Append the generated word to the list\n",
    "        generated_words.append(next_word)\n",
    "        \n",
    "        # Update the input tensor with the new word\n",
    "        input_tensor = encoder.transform(np.array([[next_word]])).toarray()\n",
    "        input_tensor = torch.Tensor(input_tensor)\n",
    "    \n",
    "    # Join the generated words into a single string\n",
    "    generated_text = ' '.join(generated_words)\n",
    "    return generated_text\n",
    "\n",
    "# Exemple d'utilisation\n",
    "generated_text = generate_text(model, encoder, start_word='Baby', num_words=20, random_sample=False)\n",
    "print(generated_text)\n",
    "\n",
    "generated_text_random = generate_text(model, encoder, start_word='Baby', num_words=20, random_sample=True)\n",
    "print(generated_text_random)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
