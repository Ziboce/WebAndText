{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contractions replaced and saved to output.txt\n"
     ]
    }
   ],
   "source": [
    "# Dictionary mapping contractions to their full forms\n",
    "contractions_dict = {\n",
    "    \"he's\": \"he is\",\n",
    "    \"i'm\": \"I am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"ain't\": \"am not\",\n",
    "    \"i've\": \"I have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"i'll\": \"I will\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"i won't\": \"I will not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"i'd\": \"I would\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"nobody's\": \"nobody is\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"we'd\": \"we would\",\n",
    "}\n",
    "\n",
    "# Function to replace contractions in the text\n",
    "def replace_contractions(text, contractions_map):\n",
    "    # Create a regex pattern that matches any of the contractions\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_map.keys()) + r')\\b')\n",
    "    # Replace contractions using the dictionary\n",
    "    return pattern.sub(lambda x: contractions_map[x.group()], text)\n",
    "\n",
    "# Function to process the text file\n",
    "def process_text_file(input_file, output_file, contractions_map):\n",
    "    # Read the contents of the input file\n",
    "    with open(input_file, 'r') as file:\n",
    "        text = file.read().lower()\n",
    "\n",
    "    # Replace contractions\n",
    "    new_text = replace_contractions(text, contractions_map)\n",
    "\n",
    "    # Write the modified text to the output file\n",
    "    # with open(output_file, 'w') as file:\n",
    "    #     file.write(new_text)\n",
    "\n",
    "    return new_text\n",
    "\n",
    "# Specify the input and output file paths\n",
    "input_file = 'adele.txt'  # Replace with the actual file path\n",
    "output_file = 'output.txt'\n",
    "\n",
    "# Process the text file\n",
    "modified_texte = process_text_file(input_file, output_file, contractions_dict)\n",
    "\n",
    "print(\"Contractions replaced and saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of train data sample: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Dimention of train_data : torch.Size([16073, 1371])\n",
      "Example of the inverted encoding: [['looking']\n",
      " ['for']\n",
      " ['some']\n",
      " ['education']\n",
      " ['made']]\n"
     ]
    }
   ],
   "source": [
    "with open(\"adele.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = modified_texte\n",
    "\n",
    "# Split the text (I consider whitespaces, points and commas as word here) + remove the empty string \"\"\n",
    "# data = np.array(list(filter(lambda a: a != \"\", re.split('(\\W)', text))))\n",
    "data = np.array(list(filter(lambda a: a != \"\", map(lambda x: x.lower(), re.split('[^a-zA-Z\\.]', text)))))\n",
    "\n",
    "# Check that the empty string is correctly removed\n",
    "if len(data[data == \"\"]) != 0:\n",
    "    raise Exception(\"The empty string wasn't proprely removed from the data\")\n",
    "\n",
    "n = int(0.8 * len(data))\n",
    "\n",
    "# Create the encoder and set the categories on the training set\n",
    "encoder = OneHotEncoder().fit(data.reshape(-1,1))\n",
    "\n",
    "# Check the number of categories of the encoder is the same than the different words in the corpus\n",
    "if len(encoder.categories_[0]) != len(set(data)):\n",
    "    raise Exception(f\"Encoder categories counts {len(encoder.categories_[0])} don't match the value of differents words {len(set(data))}\")\n",
    "\n",
    "vocab_size = len(set(data))\n",
    "\n",
    "train_data = torch.Tensor(encoder.transform(data[:n].reshape(-1,1)).toarray())\n",
    "val_data = torch.Tensor(encoder.transform(data[n:].reshape(-1,1)).toarray())\n",
    "\n",
    "print(f\"Example of train data sample: {train_data[0:5]}\")\n",
    "print(f\"Dimention of train_data : {train_data.shape}\")\n",
    "print(f\"Example of the inverted encoding: {encoder.inverse_transform(train_data[0:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcul de weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de poids : [('looking', 0.08333333333333333), ('for', 0.006896551724137931), ('some', 0.06666666666666667), ('education', 0.5), ('made', 0.03333333333333333), ('my', 0.002702702702702703), ('way', 0.023255813953488372), ('into', 0.045454545454545456), ('the', 0.0015822784810126582), ('night', 0.038461538461538464)]\n"
     ]
    }
   ],
   "source": [
    "word_counts = Counter(data)  # Compte les occurrences de chaque mot\n",
    "total_words = sum(word_counts.values())  # Nombre total de mots dans le corpus\n",
    "\n",
    "# 2. Calcul des poids inverses pour chaque mot\n",
    "word_weights = {word: 1.0 / freq for word, freq in word_counts.items()}  # Inverse de la fréquence\n",
    "\n",
    "# Normaliser les poids (optionnel, mais recommandé pour éviter des écarts extrêmes)\n",
    "max_weight = max(word_weights.values())\n",
    "word_weights = {word: weight / max_weight for word, weight in word_weights.items()}\n",
    "\n",
    "# Afficher quelques exemples de poids\n",
    "print(f\"Exemple de poids : {list(word_weights.items())[:10]}\")\n",
    "\n",
    "# Exemple : transformer les poids en un vecteur aligné avec l'encodeur\n",
    "word_to_idx = {word: idx for idx, word in enumerate(encoder.categories_[0])}  # Associer chaque mot à son index\n",
    "weights_array = np.array([word_weights[word] for word in encoder.categories_[0]])  # Créer un tableau de poids\n",
    "\n",
    "# Transformer les poids en tenseur PyTorch pour une utilisation dans la fonction de perte\n",
    "weights_tensor = torch.Tensor(weights_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is arbitrary values\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "N_EPOCHS = 10\n",
    "LR = 3e-4\n",
    "BATCH_SIZE = 32\n",
    "SEQ_LEN = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence(data, seq_len):\n",
    "    n = len(data)\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(n - seq_len):\n",
    "        X.append(data[i:i+seq_len])\n",
    "        y.append(data[i+1:i+seq_len+1])\n",
    "    return X, y\n",
    "\n",
    "train_X, train_y = create_sequence(train_data, SEQ_LEN)\n",
    "\n",
    "class Text(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "train_dataset = Text(train_X, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout, model_type='LSTM'):\n",
    "        super(NLP, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.state_dim = hidden_dim\n",
    "        self.num_layers = n_layers\n",
    "        self.rnn_type = model_type\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        if model_type == 'LSTM':\n",
    "            self.nlp = nn.LSTM(vocab_size, hidden_dim, n_layers, batch_first=True, dropout=dropout)\n",
    "        elif model_type == 'GRU':\n",
    "            self.nlp = nn.GRU(vocab_size, hidden_dim, n_layers, batch_first=True, dropout=dropout)\n",
    "        else:\n",
    "            raise Exception(\"Model type not supported\")\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        # x = [batch_size, seq_len, vocab_size]\n",
    "        # print(x.shape)\n",
    "        x, hidden = self.nlp(x)\n",
    "        # x = [batch_size, seq_len, state_dim]\n",
    "        # print(x.shape)\n",
    "        x = self.fc(x)\n",
    "        # x = [batch_size, seq_len, vocab_size]\n",
    "        # print(x.shape)\n",
    "        return x, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            # LSTM requires both hidden state and cell state\n",
    "            hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(self.device)\n",
    "            cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(self.device)\n",
    "            return (hidden, cell)\n",
    "        else:\n",
    "            # GRU only requires the hidden state\n",
    "            hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(self.device)\n",
    "            return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, n_epochs, lr, batch_size, seq_len, name):\n",
    "    # Setup GPU related variables\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"device = {device}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_losses = []\n",
    "        for i, (X, y) in enumerate(dataloader):\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output, hidden = model(X, hidden)\n",
    "\n",
    "            # print(output.shape, y.shape)\n",
    "            output = output.view(-1, vocab_size)\n",
    "            y = y.view(-1, vocab_size)\n",
    "\n",
    "            # print(output.shape, y.shape)\n",
    "            loss = criterion(output, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, step {i}, loss {loss.item()}\")\n",
    "        \n",
    "        print(f\"Epoch {epoch} finished. Train loss: {np.array(train_losses).mean()}, Perplexity: {np.exp(np.array(train_losses).mean())}\")\n",
    "\n",
    "    torch.save(model.state_dict(), f\"model_save/{name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n",
      "Epoch 0, step 0, loss 7.22035551071167\n",
      "Epoch 0, step 100, loss 5.502964973449707\n",
      "Epoch 0, step 200, loss 5.4336934089660645\n",
      "Epoch 0, step 300, loss 5.363304615020752\n",
      "Epoch 0, step 400, loss 5.403769016265869\n",
      "Epoch 0, step 500, loss 5.500420570373535\n",
      "Epoch 0 finished. Train loss: 5.540769781249453, Perplexity: 254.87412128295293\n",
      "Epoch 1, step 0, loss 5.523467540740967\n",
      "Epoch 1, step 100, loss 5.390746593475342\n",
      "Epoch 1, step 200, loss 5.520390033721924\n",
      "Epoch 1, step 300, loss 5.386978626251221\n",
      "Epoch 1, step 400, loss 5.3468337059021\n",
      "Epoch 1, step 500, loss 5.40149450302124\n",
      "Epoch 1 finished. Train loss: 5.410252050574557, Perplexity: 223.6879612549074\n",
      "Epoch 2, step 0, loss 5.258214473724365\n",
      "Epoch 2, step 100, loss 5.454404830932617\n",
      "Epoch 2, step 200, loss 5.186992168426514\n",
      "Epoch 2, step 300, loss 5.192572116851807\n",
      "Epoch 2, step 400, loss 5.061707496643066\n",
      "Epoch 2, step 500, loss 5.006148338317871\n",
      "Epoch 2 finished. Train loss: 5.169679987478066, Perplexity: 175.85855153986023\n",
      "Epoch 3, step 0, loss 5.090599060058594\n",
      "Epoch 3, step 100, loss 4.8658599853515625\n",
      "Epoch 3, step 200, loss 4.65172815322876\n",
      "Epoch 3, step 300, loss 4.503186225891113\n",
      "Epoch 3, step 400, loss 4.153320789337158\n",
      "Epoch 3, step 500, loss 4.084997653961182\n",
      "Epoch 3 finished. Train loss: 4.479408002944582, Perplexity: 88.18245346757547\n",
      "Epoch 4, step 0, loss 3.9856369495391846\n",
      "Epoch 4, step 100, loss 3.658343553543091\n",
      "Epoch 4, step 200, loss 3.4526174068450928\n",
      "Epoch 4, step 300, loss 3.1201655864715576\n",
      "Epoch 4, step 400, loss 3.0190155506134033\n",
      "Epoch 4, step 500, loss 2.898815631866455\n",
      "Epoch 4 finished. Train loss: 3.2940141746247433, Perplexity: 26.950832155266692\n",
      "Epoch 5, step 0, loss 2.766266345977783\n",
      "Epoch 5, step 100, loss 2.6424789428710938\n",
      "Epoch 5, step 200, loss 2.6263375282287598\n",
      "Epoch 5, step 300, loss 2.1898748874664307\n",
      "Epoch 5, step 400, loss 2.178976535797119\n",
      "Epoch 5, step 500, loss 1.808417558670044\n",
      "Epoch 5 finished. Train loss: 2.2797572930970516, Perplexity: 9.774307829637998\n",
      "Epoch 6, step 0, loss 1.8029496669769287\n",
      "Epoch 6, step 100, loss 1.879797339439392\n",
      "Epoch 6, step 200, loss 1.843505620956421\n",
      "Epoch 6, step 300, loss 1.6029521226882935\n",
      "Epoch 6, step 400, loss 1.5741218328475952\n",
      "Epoch 6, step 500, loss 1.5779812335968018\n",
      "Epoch 6 finished. Train loss: 1.6466964701732316, Perplexity: 5.18980679544254\n",
      "Epoch 7, step 0, loss 1.5266505479812622\n",
      "Epoch 7, step 100, loss 1.560027837753296\n",
      "Epoch 7, step 200, loss 1.3445755243301392\n",
      "Epoch 7, step 300, loss 1.254974365234375\n",
      "Epoch 7, step 400, loss 1.2138183116912842\n",
      "Epoch 7, step 500, loss 0.9407497048377991\n",
      "Epoch 7 finished. Train loss: 1.2534479110601888, Perplexity: 3.5023981201704073\n",
      "Epoch 8, step 0, loss 1.1660914421081543\n",
      "Epoch 8, step 100, loss 0.8419511318206787\n",
      "Epoch 8, step 200, loss 1.0410676002502441\n",
      "Epoch 8, step 300, loss 0.8777183890342712\n",
      "Epoch 8, step 400, loss 0.9133917093276978\n",
      "Epoch 8, step 500, loss 1.0415904521942139\n",
      "Epoch 8 finished. Train loss: 0.9975317299840935, Perplexity: 2.711580648490035\n",
      "Epoch 9, step 0, loss 0.9340284466743469\n",
      "Epoch 9, step 100, loss 0.8989540934562683\n",
      "Epoch 9, step 200, loss 0.7785291075706482\n",
      "Epoch 9, step 300, loss 0.7615630626678467\n",
      "Epoch 9, step 400, loss 0.7221507430076599\n",
      "Epoch 9, step 500, loss 0.6615619659423828\n",
      "Epoch 9 finished. Train loss: 0.8206029206633093, Perplexity: 2.2718691815619185\n"
     ]
    }
   ],
   "source": [
    "model_type = 'GRU'\n",
    "GRU_model = NLP(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT, model_type)\n",
    "\n",
    "train(GRU_model, train_loader, N_EPOCHS, LR, BATCH_SIZE, SEQ_LEN, f\"{model_type}_model_OneHot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n",
      "Epoch 0, step 0, loss 7.230099201202393\n",
      "Epoch 0, step 100, loss 5.658360958099365\n",
      "Epoch 0, step 200, loss 5.602745532989502\n",
      "Epoch 0, step 300, loss 5.541574478149414\n",
      "Epoch 0, step 400, loss 5.453268051147461\n",
      "Epoch 0, step 500, loss 5.594719409942627\n",
      "Epoch 0 finished. Train loss: 5.5737400339894085, Perplexity: 263.4174494267315\n",
      "Epoch 1, step 0, loss 5.398975849151611\n",
      "Epoch 1, step 100, loss 5.377743721008301\n",
      "Epoch 1, step 200, loss 5.397225856781006\n",
      "Epoch 1, step 300, loss 5.416540622711182\n",
      "Epoch 1, step 400, loss 5.345911979675293\n",
      "Epoch 1, step 500, loss 5.3409342765808105\n",
      "Epoch 1 finished. Train loss: 5.425348689356648, Perplexity: 227.09051655768465\n",
      "Epoch 2, step 0, loss 5.370179653167725\n",
      "Epoch 2, step 100, loss 5.455815315246582\n",
      "Epoch 2, step 200, loss 5.388798236846924\n",
      "Epoch 2, step 300, loss 5.473689556121826\n",
      "Epoch 2, step 400, loss 5.313849449157715\n",
      "Epoch 2, step 500, loss 5.387132167816162\n",
      "Epoch 2 finished. Train loss: 5.3990394689172385, Perplexity: 221.19385056379562\n",
      "Epoch 3, step 0, loss 5.223694324493408\n",
      "Epoch 3, step 100, loss 5.26630973815918\n",
      "Epoch 3, step 200, loss 5.231741428375244\n",
      "Epoch 3, step 300, loss 5.1244940757751465\n",
      "Epoch 3, step 400, loss 5.163646221160889\n",
      "Epoch 3, step 500, loss 5.074714183807373\n",
      "Epoch 3 finished. Train loss: 5.184664352006647, Perplexity: 178.51352202761674\n",
      "Epoch 4, step 0, loss 4.940031051635742\n",
      "Epoch 4, step 100, loss 4.890470504760742\n",
      "Epoch 4, step 200, loss 4.820021629333496\n",
      "Epoch 4, step 300, loss 4.769245624542236\n",
      "Epoch 4, step 400, loss 4.794933795928955\n",
      "Epoch 4, step 500, loss 4.490873336791992\n",
      "Epoch 4 finished. Train loss: 4.857560020993906, Perplexity: 128.70976952340777\n",
      "Epoch 5, step 0, loss 4.824832439422607\n",
      "Epoch 5, step 100, loss 4.635725975036621\n",
      "Epoch 5, step 200, loss 4.481657028198242\n",
      "Epoch 5, step 300, loss 4.635798931121826\n",
      "Epoch 5, step 400, loss 4.389466285705566\n",
      "Epoch 5, step 500, loss 4.650553226470947\n",
      "Epoch 5 finished. Train loss: 4.571414882918278, Perplexity: 96.68080506573594\n",
      "Epoch 6, step 0, loss 4.472289085388184\n",
      "Epoch 6, step 100, loss 4.486974239349365\n",
      "Epoch 6, step 200, loss 4.264710426330566\n",
      "Epoch 6, step 300, loss 4.346357345581055\n",
      "Epoch 6, step 400, loss 4.249392509460449\n",
      "Epoch 6, step 500, loss 4.0683913230896\n",
      "Epoch 6 finished. Train loss: 4.292395937490273, Perplexity: 73.14150119701323\n",
      "Epoch 7, step 0, loss 4.00175142288208\n",
      "Epoch 7, step 100, loss 4.044042110443115\n",
      "Epoch 7, step 200, loss 4.0317063331604\n",
      "Epoch 7, step 300, loss 3.907933235168457\n",
      "Epoch 7, step 400, loss 3.850297212600708\n",
      "Epoch 7, step 500, loss 3.7982335090637207\n",
      "Epoch 7 finished. Train loss: 4.011389656845792, Perplexity: 55.22355906267392\n",
      "Epoch 8, step 0, loss 3.8559107780456543\n",
      "Epoch 8, step 100, loss 3.7817463874816895\n",
      "Epoch 8, step 200, loss 3.9329049587249756\n",
      "Epoch 8, step 300, loss 3.7663962841033936\n",
      "Epoch 8, step 400, loss 3.746331214904785\n",
      "Epoch 8, step 500, loss 3.6625964641571045\n",
      "Epoch 8 finished. Train loss: 3.7387397920942878, Perplexity: 42.04497135935209\n",
      "Epoch 9, step 0, loss 3.6359455585479736\n",
      "Epoch 9, step 100, loss 3.6920344829559326\n",
      "Epoch 9, step 200, loss 3.45436954498291\n",
      "Epoch 9, step 300, loss 3.4697930812835693\n",
      "Epoch 9, step 400, loss 3.675016164779663\n",
      "Epoch 9, step 500, loss 3.4032084941864014\n",
      "Epoch 9 finished. Train loss: 3.473768481220382, Perplexity: 32.25807763624109\n"
     ]
    }
   ],
   "source": [
    "model_type = 'LSTM'\n",
    "LSTM_model = NLP(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT, model_type)\n",
    "\n",
    "train(LSTM_model, train_loader, N_EPOCHS, LR, BATCH_SIZE, SEQ_LEN, f\"{model_type}_model_OneHot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, encoder, start_word, num_words=10, random_sample=False):\n",
    "    \"\"\"\n",
    "    Generate text based on the trained model output.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The trained PyTorch model.\n",
    "    - encoder: The OneHotEncoder used for encoding the words.\n",
    "    - start_word: The initial word to start generating text.\n",
    "    - num_words: Number of words to generate.\n",
    "    - random_sample: If True, sample from the distribution instead of taking the max probability.\n",
    "    \n",
    "    Returns:\n",
    "    - generated_text: The generated sequence of words.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    start_word = start_word.lower().split()\n",
    "    # Initialize the generated text with the start word\n",
    "    generated_words = start_word\n",
    "    \n",
    "    # Convert the start word to its one-hot encoded representation\n",
    "    input_word = encoder.transform(np.array(start_word).reshape(-1, 1)).toarray()\n",
    "    input_tensor = torch.Tensor(input_word).unsqueeze(0).to(model.device)  # Add batch dimension\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    \n",
    "    # Generate the specified number of words\n",
    "    for _ in range(num_words):\n",
    "        # Get the model output with the hidden state\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(input_tensor, hidden)  # Pass hidden state\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = torch.softmax(output, dim=-1).squeeze().cpu().numpy()\n",
    "\n",
    "        if len(probabilities.shape) != 2:\n",
    "            probabilities = probabilities.reshape((1,-1))\n",
    "\n",
    "        model_output = []\n",
    "        for probability in probabilities:\n",
    "            # Determine the next word\n",
    "            if random_sample:\n",
    "                next_index = np.random.choice(len(probability), p=probability)\n",
    "            else:\n",
    "                next_index = np.argmax(probability)\n",
    "\n",
    "            encoding = np.zeros_like(probability)\n",
    "            encoding[next_index] = 1\n",
    "            # Get the corresponding word from the encoder\n",
    "            next_word = encoder.inverse_transform(encoding.reshape(1, -1))[0][0]\n",
    "            \n",
    "            # Append the generated word to the list\n",
    "            model_output.append(next_word)\n",
    "\n",
    "        for word in model_output:\n",
    "            generated_words.append(word)\n",
    "\n",
    "        # Update the input tensor with the new word\n",
    "        input_tensor = encoder.transform(np.array(model_output).reshape(-1, 1)).toarray()\n",
    "        input_tensor = torch.Tensor(input_tensor).unsqueeze(0).to(model.device)  # Add batch dimension\n",
    "    \n",
    "    # Join the generated words into a single string\n",
    "    generated_text = ' '.join(generated_words)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so i will be\n",
      "i know there is no will i is a tomorrow be see a child all\n",
      "i m will keep be up\n",
      "baby s it help\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'utilisation\n",
    "generated_text = generate_text(GRU_model, encoder, start_word='So', num_words=3, random_sample=False)\n",
    "print(generated_text)\n",
    "\n",
    "generated_text = generate_text(GRU_model, encoder, start_word='I know there is no', num_words=2, random_sample=False)\n",
    "print(generated_text)\n",
    "\n",
    "generated_text = generate_text(GRU_model, encoder, start_word='I m', num_words=2, random_sample=False)\n",
    "print(generated_text)\n",
    "\n",
    "generated_text_random = generate_text(GRU_model, encoder, start_word='Baby', num_words=3, random_sample=True)\n",
    "print(generated_text_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so i am i\n",
      "i know there is no am you are gonna waste i love a wish no\n",
      "i m am not i love\n",
      "baby are harder got\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'utilisation\n",
    "generated_text = generate_text(LSTM_model, encoder, start_word='So', num_words=3, random_sample=False)\n",
    "print(generated_text)\n",
    "\n",
    "generated_text = generate_text(LSTM_model, encoder, start_word='I know there is no', num_words=2, random_sample=False)\n",
    "print(generated_text)\n",
    "\n",
    "generated_text = generate_text(LSTM_model, encoder, start_word='I m', num_words=2, random_sample=False)\n",
    "print(generated_text)\n",
    "\n",
    "generated_text_random = generate_text(LSTM_model, encoder, start_word='Baby', num_words=3, random_sample=True)\n",
    "print(generated_text_random)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
